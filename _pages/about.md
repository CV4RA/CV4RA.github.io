---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about'></span>

Li Zhenyu (ææŒ¯é›¨), Ph.D in Mechanical Engineering of Tongji University, jointed the Department of Robotics Engineering of Qilu University of Technology (Shandong Academy of Sciences) from Oct. 2023, and built the computer vision for robot automation laboratory (CV4RA lab.). He has published more than 30 papers in SCI journals and robotics academic conferences. He achieved the Best Paper Finalist Award at the "2019 IEEE ROBIO Conference" (<font face="åæ–‡æ–°é­" color="red">Top 1.5%</font>). He was awarded the "Outstanding Graduate of Tongji University" in 2023 (<font face="åæ–‡æ–°é­" color="red">Top 2%</font>). He currently serves as a reviewer for several TOP journals (i.e., IEEE-TII, IEEE-T-ITS, IEEE-RAL, ...). His current research covers localization and navigation for intelligent unmanned systems (including robots, autonomous vehicles, UAVs, etc.).

ğŸ“£ <font color=gray face="åæ–‡æ–°é­">We welcome outstanding and self-motivating undergraduate students (second-year, third-year, and graduate students) to participate in subject competitions, and (sophomore students and above) to jointly carry out research on cutting-edge topics. We also welcome outstanding postgraduate students from both inside and outside the school to collaborate on AI & Robotic research.</font>

<b><font face="æ¥·ä¹¦">Available Research Topics (for prospective students):</font></b>
- <font face="åæ–‡æ–°é­" color="Hotpink">Cross-domain adaptive visual place recognition for mobile robots</font>
- <font face="åæ–‡æ–°é­" color="Hotpink">Multi-modal Perception-based automatic navigation for intelligent vehicles</font>
- <font face="åæ–‡æ–°é­" color="Hotpink">Multi-Robot-Collaboration SLAM</font>
- <font face="åæ–‡æ–°é­" color="Hotpink">Heterogeneous multi-agent collaboration for visual place recognition</font>
- <font face="åæ–‡æ–°é­" color="Hotpink">Multi-UAVs visual perception with low-illumination environments</font>
- <font face="åæ–‡æ–°é­" color="Hotpink">Image quality analysis (fog, rain or snow removal technology)</font>

#  Research Topics
- Robot Localization and Navigation
- Cross-modal Perception
- Intelligent Driving 
- Edge Intelligent Computing

ğŸ“¤<u><font size=2 color=Hotpink>æœ‰å¿—äºåœ¨CV4RAä»äº‹å­¦æœ¯ç ”ç©¶å’Œå­¦ç§‘ç«èµ›çš„ä¼˜ç§€åŒå­¦(<b>ç‰¹åˆ«æ¬¢è¿æ‰“ç®—å‡ºå›½æ·±é€ å’Œè€ƒç ”å­¦ç”ŸåŠ å…¥</b>)ï¼Œæ¬¢è¿è”ç³»æˆ‘æ´½è°ˆï¼ Email(å—–~ï¼ğŸš€å¯è¾¾)ï¼šlizhenyu@ieee.org.</font></u>

#  News
- *2025.01*: &nbsp;![alt text](new-3.gif) Our paper "CWPFormer: Towards High-performance Visual Place Recognition for Robot with Cross-weight Attention Learning" has been accepted byã€ŠIEEE Transactions on Artificial Intelligenceã€‹ ! <font color=Fuchsia>well doneğŸ‘ğŸ‘ğŸ‘!!!</font>
- *2025.01*: &nbsp;![alt text](new-3.gif) Our paper "Multi-Modal Attention Perception for Intelligent Vehicle Navigation using Deep Reinforcement Learning" has been accepted byã€ŠIEEE Transactions on Intelligent Transportation Systemsã€‹ ! <font color=Fuchsia>well doneğŸ‘ğŸ‘ğŸ‘!!!</font>
- *2024.11*: &nbsp;![alt text](new-3.gif) Our paper "Feature-Level Knowledge Distillation for Place Recognition based on Soft-Hard Labels Teaching Paradigm" has been accepted byã€ŠIEEE Transactions on Intelligent Transportation Systemsã€‹ ! <font color=Fuchsia>well doneğŸ‘ğŸ‘ğŸ‘!!!</font>
- *2024.10*: &nbsp;![alt text](new-2.gif) Our paper "Towards to Robust Visual Place Recognition for Mobile Robots with an End-to-end Dark-enhanced Net" has been accepted byã€ŠIEEE Transactions on Industrial Informaticsã€‹! <font color=Fuchsia>well doneğŸ‘ğŸ‘ğŸ‘!!!</font> 
- *2024.08*: &nbsp;![alt text](new-3.gif) Our work "Intelligent Walker - Autonomous Navigation Mobile Robot Based on Intelligent Perception" has been identified as a proposed project for the "2024 Provincial College Students' Innovation and Entrepreneurship Training Program" ! [(é“¾æ¥)](http://edu.shandong.gov.cn/module/download/downfile.jsp?classid=0&filename=c1e5787543f146ea8e779ea0ae86a679.pdf) (<font color=DeepSkyBlue>no. 32</font>)
- *2024.07*: &nbsp;![alt text](new-3.gif) Our paper "Pyramid Transformer-based Triplet Hashing for Robust Visual Place Recognition" has been accepted after minor revision byã€ŠComputer Vision and Image Understandingã€‹ ! 
- *2024.06*: &nbsp;![alt text](new-2.gif) Our work "Time Series Prediction Model Based on Transformer Exhaust Emissions" won the provincial first prize in the "2024 National College Student Statistical Modeling Competition" ! [(é“¾æ¥)](http://cmswebsite.ai-learning.net/u/cms/tjjmds/202407/11160343pa3f.pdf) (<font color=DeepSkyBlue>no. 1302</font>)
- *2024.06*: &nbsp;![alt text](new-2.gif) Our paper "Reinforcement learning-based distributed impedance control of robots for compliant operation in tight interaction tasks" has been accepted byã€ŠEngineering Applications of Artificial Intelligenceã€‹! <font color=Fuchsia>well doneğŸ‘ğŸ‘ğŸ‘!!!</font> 
- *2024.03*: &nbsp;![alt text](new-2.gif) Our paper "CSPFormer: A Cross-Spatial Pyramid Transformer for Visual Place Recognition" has been accepted byã€ŠNeurocomputingã€‹! 
- *2024.02*: &nbsp;![alt text](new-3.gif) Our paper "TECD_Attention: Texture-enhanced and cross-domain attention modeling for visual place recognition" has been accepted byã€ŠComputer Vision and Image Understandingã€‹ ! 

#  Honors and Awards
- *2024.08* 2024å¹´çœçº§å¤§å­¦ç”Ÿåˆ›æ–°åˆ›ä¸šè®­ç»ƒè®¡åˆ’é¡¹ç›®æ‹Ÿç«‹é¡¹ (æŒ‡å¯¼è€å¸ˆ)
- *2024.06* 2024å¹´ï¼ˆç¬¬åå±Šï¼‰å…¨å›½å¤§å­¦ç”Ÿç»Ÿè®¡å»ºæ¨¡å¤§èµ›å±±ä¸œèµ›åŒºä¸€ç­‰å¥– (æŒ‡å¯¼è€å¸ˆ)
- *2019.12* Best Paper Finalist in 2019 IEEE-ROBIO conference 
- *2019.10* Excellent Doctoral Scholarship in 2019 Tongji University
- *2023.06* Outstanding graduates in 2023 Tongji University

#  Publications 
<font color=gray face="åæ–‡æ–°é­">(2025 submitted papers)
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE-TCSVT</div><img src='images/tgrs.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Text-Driven Cross-Modal Place Recognition Method for Remote Sensing Localization ](https://arxiv.org/pdf/2503.18035), *IEEE-TCSVT*.

 <a href="https://arxiv.org/pdf/2503.18035"><img src="https://img.shields.io/badge/Paper-pdf-<COLOR>.svg?style=flat-square" /></a> [![Code](https://img.shields.io/badge/GitHub-Code-lightgrey?logo=github)](https://github.com/CV4RA/des4pos)

Tianyi Shang, **Zhenyu Li***, Pengjie Xu, Zhaojun Deng, Ruirui Zhang
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">2025-IROS</div><img src='images/text4vpr.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Bridging Text and Vision: A Multi-View Text-Vision Registration Approach for Cross-Modal Place Recognition](https://arxiv.org/pdf/2502.14195), *2025-IROS*.

 <a href="https://arxiv.org/pdf/2502.14195"><img src="https://img.shields.io/badge/Paper-pdf-<COLOR>.svg?style=flat-square" /></a> [![Code](https://img.shields.io/badge/GitHub-Code-lightgrey?logo=github)](https://github.com/nuozimiaowu/Text4VPR)

Tianyi Shang, **Zhenyu Li***, Pengjie Xu, Jinwei Qiaoï¼ŒGang Chen, Zihan Ruan, Weijun Hu
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">2025-IROS</div><img src='images/ral.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[MambaPlace: Text-to-Point-Cloud Cross-Modal Place Recognition with Attention Mamba Mechanisms](https://arxiv.org/pdf/2408.15740), *2025-IROS*.

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/mambaplace-text-to-point-cloud-cross-modal/visual-place-recognition-on-kitti360pose)](https://paperswithcode.com/sota/visual-place-recognition-on-kitti360pose?p=mambaplace-text-to-point-cloud-cross-modal) <a href="https://arxiv.org/pdf/2408.15740"><img src="https://img.shields.io/badge/Paper-pdf-<COLOR>.svg?style=flat-square" /></a> [![Code](https://img.shields.io/badge/GitHub-Code-lightgrey?logo=github)](https://github.com/CV4RA/MambaPlace)

Tianyi Shang, **Zhenyu Li***, Pengjie Xu, Jinwei Qiao
</div>
</div>
<font color=gray face="åæ–‡æ–°é­">(2025 published papers)
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE-TITS</div><img src='images/its.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Multi-Modal Attention Perception for Intelligent Vehicle Navigation using Deep Reinforcement Learning](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10877697), *IEEE Transactions on Intelligent Transportation Systems*. 

<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10877697"><img src="https://img.shields.io/badge/Paper-pdf-<COLOR>.svg?style=flat-square" /></a> [![Code](https://img.shields.io/badge/GitHub-Code-lightgrey?logo=github)](https://github.com/CV4RA/MMAP-DRL-Nav)

**Zhenyu Li***, Tianyi Shang, Pengjie Xu
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE-TAI</div><img src='images/tai.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[CWPFormer: Towards High-performance Visual Place Recognition for Robot with Cross-weight Attention Learning](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10872972), *IEEE Transactions on Artificial Intelligence*. 

<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10872972"><img src="https://img.shields.io/badge/Paper-pdf-<COLOR>.svg?style=flat-square" /></a>  [![Code](https://img.shields.io/badge/GitHub-Code-lightgrey?logo=github)](https://github.com/CV4RA/CWPFormer)

**Zhenyu Li***, Pengjie Xu, Tianyi Shang
</div>
</div>

<font color=gray face="åæ–‡æ–°é­">(2024 published papers)

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE-TITS</div><img src='images/itits.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Feature-Level Knowledge Distillation for Place Recognition based on Soft-Hard Labels Teaching Paradigm](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10759546), *IEEE Transactions on Intelligent Transportation Systems*. (ä¸­ç§‘é™¢1åŒºtop)

[![Paper](https://img.shields.io/badge/Paper-pdf-brightgreen)](_pages/itits.pdf) [![Code](https://img.shields.io/badge/GitHub-Code-lightgrey?logo=github)](https://github.com/CV4RA/ASHT-KD)

**Zhenyu Li***, Pengjie Xu, Zhenbiao Dong, Ruirui Zhang, Zhaojun Deng
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE-TII</div><img src='images/tii.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Towards Robust Visual Place Recognition for Mobile Robots with an End-to-end Dark-enhanced Net](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10726589), *IEEE Transactions on Industrial Informatics*. (ä¸­ç§‘é™¢1åŒºtop)

[![Paper](https://img.shields.io/badge/Paper-pdf-brightgreen)](_pages/FINALVERSION.pdf) [![Code](https://img.shields.io/badge/GitHub-Code-lightgrey?logo=github)](https://github.com/CV4RA/Dark-enhanced-VPR-Net)

**Zhenyu Li***, Tianyi Shang, Pengjie Xu, Zhaojun Deng, and Ruirui Zhang
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVIU</div><img src='images/cviu.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Pyramid transformer-based triplet hashing for robust visual place recognition](https://www.sciencedirect.com/science/article/pii/S1077314224002480), *Computer Vision and Image Understanding*. (CCF-B)

**Zhenyu Li*** and Pengjie Xu
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EAAI</div><img src='images/eaai.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Reinforcement learning-based distributed impedance control of robots forcompliant operation in tight interaction tasks](https://authors.elsevier.com/c/1jPYU3OWJ98fVS), *Engineering Applications of Artificial Intelligence*. (ä¸­ç§‘é™¢1åŒºtop)

Pengjie Xu, **Zhenyu Li**, Xun Liu, Tianrui Zhao, Lin Zhang, Yanzheng Zhao
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Neurocomputing</div><img src='images/CSPFormer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[CSPFormer: A Cross-Spatial Pyramid Transformer for Visual Place Recognition](https://www.sciencedirect.com/science/article/pii/S0925231224002431), *Neurocomputing*. (ä¸­ç§‘é™¢äºŒåŒºtop, CCF-C)

**Zhenyu Li***, Pengjie Xu
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVIU</div><img src='images/TECD.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[TECD_Attention: Texture-enhanced and cross-domain attention modeling for visual place recognition](https://www.sciencedirect.com/science/article/pii/S1077314224000109), *Computer Vision and Image Understanding*. (CCF-B)

**Zhenyu Li***, Zhenbiao Dong
</div>
</div>


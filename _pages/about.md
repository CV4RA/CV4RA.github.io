---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Li Zhenyu, Ph.D in Mechanical Engineering of Tongji University, is a teacher in the Department of Robotics Engineering at Qilu University of Technology (Shandong Academy of Sciences), and a leader of computer vision for robot automation laboratory (CV4RA lab.). He has published more than 30 papers in SCI journals and robotics academic conferences. He achieved the Best Paper Finalist Award at the 2019 IEEE ROBIO Conference. He currently serves as a reviewer for several TOP journals. His current research covers localization and navigation for intelligent unmanned systems (including robots, autonomous vehicles, UAVs, etc.).

ğŸ“£ <font color=gray face="åæ–‡æ–°é­">We welcome outstanding and self-motivating undergraduate students (first-year students and above) to participate in subject competitions, and (sophomore students and above) to jointly carry out research on cutting-edge topics. We also welcome outstanding postgraduate students from both inside and outside the school to collaborate on AI & Robotic research.</font>

<b><font face="æ¥·ä¹¦">Available Research Topics (for prospective students):</font></b>
- <font face="åæ–‡æ–°é­" color="Hotpink">Cross-domain adaptive visual place recognition for mobile robots</font>
- <font face="åæ–‡æ–°é­" color="Hotpink">Multi-modal Perception-based automatic navigation for intelligent vehicles</font>
- <font face="åæ–‡æ–°é­" color="Hotpink">3D semantic SLAM</font>
- <font face="åæ–‡æ–°é­" color="Hotpink">Heterogeneous multi-agent collaboration for visual place recognition</font>
- <font face="åæ–‡æ–°é­" color="Hotpink">Multi-UAVs visual perception with low-illumination environments</font>
- <font face="åæ–‡æ–°é­" color="Hotpink">Image quality analysis (fog, rain or snow removal technology)</font>

# ğŸ° Research Field
- Intelligent Scene Perception (semantic understanding)
- Robot Visual Place Recognition (2D & 3D VPR)
- Intelligent Driving (single/multi-agents)
- Edge Computing (GPU-drive intelligent computing)

ğŸ“¤<u><font size=2 color=Hotpink>æœ‰å¿—äºåœ¨CV4RAä»äº‹å­¦æœ¯ç ”ç©¶å’Œå­¦ç§‘ç«èµ›çš„ä¼˜ç§€åŒå­¦(<b>ç‰¹åˆ«æ¬¢è¿æ‰“ç®—å‡ºå›½æ·±é€ å’Œè€ƒç ”å­¦ç”ŸåŠ å…¥</b>)ï¼Œæ¬¢è¿è”ç³»æˆ‘æ´½è°ˆï¼ Email(å—–~ï¼ğŸš€å¯è¾¾)ï¼šlizhenyu@qlu.edu.cn.</font></u>

# ğŸ“ˆ Projects
- "Research on Key Fundamental Issues of Perception, Decision Making and Control of Intelligent Electric Vehicles â†’ Intelligent electric vehicle information security guarantee theory and prevention", National key research and development plan sub-project.
- "DPF (Diesel Particulate Filter) equipment Tongji Platform data monitoring and data analysis", Shanghai Transportation Commission.

# ğŸ”¥ News
- *2024.03*: &nbsp;![alt text](new-2.gif) Our paper "CSPFormer: A Cross-Spatial Pyramid Transformer for Visual Place Recognition" has been accepted by Neurocomputing ! 
- *2024.02*: &nbsp;![alt text](new-3.gif)Our paper "TECD_Attention: Texture-enhanced and cross-domain attention modeling for visual place recognition" has been accepted by Computer Vision and Image Understanding ! 

# ğŸ– Honors and Awards
- *2019.12* Best Paper Finalist in 2019 IEEE-ROBIO conference. 
- *2019.10* Excellent Doctoral Scholarship in 2019 Tongji University. 
- *2023.06* Outstanding graduates in 2023 Tongji University.

# ğŸ“– Experiences
- *2023.10 - present*, Teacher, School of Mechanical Engineering, Qilu University of Technology (Shandong Academy of Sciences). 
- *2018.09 - 2023.06*, Ph.D candidate, Tongji University. 

# ğŸ“ Publications 
<font color=gray face="åæ–‡æ–°é­">(selected publications)

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Neurocomputing</div><img src='images/CSPFormer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[CSPFormer: A Cross-Spatial Pyramid Transformer for Visual Place Recognition](https://www.sciencedirect.com/science/article/pii/S0925231224002431), Neurocomputing.

**Zhenyu Li**, Pengjie Xu
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVIU</div><img src='images/TECD.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[TECD_Attention: Texture-enhanced and cross-domain attention modeling for visual place recognition](https://www.sciencedirect.com/science/article/pii/S1077314224000109), Computer Vision and Image Understanding.

**Zhenyu Li**, Zhenbiao Dong
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AI</div><img src='images/RDDRL.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[RDDRL: a recurrent deduction deep reinforcement learning model for multimodal vision-robot navigation](https://link.springer.com/article/10.1007/s10489-023-04754-7), Applied Intelligence.

**Zhenyu Li**, Aiguo Zhou
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AI</div><img src='images/TSF.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[TSF-transformer: a time series forecasting model for exhaust gas emission using transformer](https://link.springer.com/article/10.1007/s10489-022-04326-1), Applied Intelligence.

**Zhenyu Li**, Xikun, Zhenbiao Dong
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">JIRS</div><img src='images/self.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Self-selection salient region-based scene recognition using slight-weight convolutional neural network](https://link.springer.com/article/10.1007/s10846-021-01421-2), Journal of Intelligent & Robotic Systems.

**Zhenyu Li**, Aiguo Zhou
</div>
</div>

# ğŸ’Œ Contact us
- email: lizhenyu@qlu.edu.cn

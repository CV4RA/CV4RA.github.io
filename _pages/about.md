---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about'></span>

Li Zhenyu (李振雨), Ph.D in Mechanical Engineering of Tongji University, jointed the Department of Robotics Engineering of Qilu University of Technology (Shandong Academy of Sciences) from 2023, and built the computer vision for robot automation laboratory (CV4RA lab.). He has published more than 30 papers in SCI journals and robotics academic conferences. He achieved the Best Paper Finalist Award at the "2019 IEEE ROBIO Conference" (<font face="华文新魏" color="red">Top 0.5%</font>). He was awarded the "Outstanding Graduate of Tongji University" in 2023 (<font face="华文新魏" color="red">Top 2%</font>). He currently serves as a reviewer for several TOP journals (i.e., IEEE-TII, IEEE-TMECH, EAAI, ...). His current research covers localization and navigation for intelligent unmanned systems (including robots, autonomous vehicles, UAVs, etc.).

📣 <font color=gray face="华文新魏">We welcome outstanding and self-motivating undergraduate students (first-year students and above) to participate in subject competitions, and (sophomore students and above) to jointly carry out research on cutting-edge topics. We also welcome outstanding postgraduate students from both inside and outside the school to collaborate on AI & Robotic research.</font>

<b><font face="楷书">Available Research Topics (for prospective students):</font></b>
- <font face="华文新魏" color="Hotpink">Cross-domain adaptive visual place recognition for mobile robots</font>
- <font face="华文新魏" color="Hotpink">Multi-modal Perception-based automatic navigation for intelligent vehicles</font>
- <font face="华文新魏" color="Hotpink">3D semantic SLAM</font>
- <font face="华文新魏" color="Hotpink">Heterogeneous multi-agent collaboration for visual place recognition</font>
- <font face="华文新魏" color="Hotpink">Multi-UAVs visual perception with low-illumination environments</font>
- <font face="华文新魏" color="Hotpink">Image quality analysis (fog, rain or snow removal technology)</font>

#  Research Topics
- Intelligent Scene Perception
- Robot Visual Place Recognition
- Intelligent Driving 
- Edge Intelligent Computing

📤<u><font size=2 color=Hotpink>有志于在CV4RA从事学术研究和学科竞赛的优秀同学(<b>特别欢迎打算出国深造和考研学生加入</b>)，欢迎联系我洽谈！ Email(嗖~！🚀可达)：lizhenyu@ieee.org.</font></u>

#  News
- *2024.11*: &nbsp;![alt text](new-3.gif) Our paper "Feature-Level Knowledge Distillation for Place Recognition based on Soft-Hard Labels Teaching Paradigm" has been accepted by《IEEE Transactions on Intelligent Transportation Systems》 ! <font color=Fuchsia>well done👏👏👏!!!</font>
- *2024.10*: &nbsp;![alt text](new-2.gif) Our paper "Towards to Robust Visual Place Recognition for Mobile Robots with an End-to-end Dark-enhanced Net" has been accepted by《IEEE Transactions on Industrial Informatics》! <font color=Fuchsia>well done👏👏👏!!!</font> 
- *2024.08*: &nbsp;![alt text](new-3.gif) Our work "Intelligent Walker - Autonomous Navigation Mobile Robot Based on Intelligent Perception" has been identified as a proposed project for the "2024 Provincial College Students' Innovation and Entrepreneurship Training Program" ! [(链接)](http://edu.shandong.gov.cn/module/download/downfile.jsp?classid=0&filename=c1e5787543f146ea8e779ea0ae86a679.pdf) (<font color=DeepSkyBlue>no. 32</font>)
- *2024.07*: &nbsp;![alt text](new-3.gif) Our paper "Pyramid Transformer-based Triplet Hashing for Robust Visual Place Recognition" has been accepted after minor revision by《Computer Vision and Image Understanding》 ! 
- *2024.06*: &nbsp;![alt text](new-2.gif) Our work "Time Series Prediction Model Based on Transformer Exhaust Emissions" won the provincial first prize in the "2024 National College Student Statistical Modeling Competition" ! [(链接)](http://cmswebsite.ai-learning.net/u/cms/tjjmds/202407/11160343pa3f.pdf) (<font color=DeepSkyBlue>no. 1302</font>)
- *2024.06*: &nbsp;![alt text](new-2.gif) Our paper "Reinforcement learning-based distributed impedance control of robots for compliant operation in tight interaction tasks" has been accepted by《Engineering Applications of Artificial Intelligence》! <font color=Fuchsia>well done👏👏👏!!!</font> 
- *2024.03*: &nbsp;![alt text](new-2.gif) Our paper "CSPFormer: A Cross-Spatial Pyramid Transformer for Visual Place Recognition" has been accepted by《Neurocomputing》! 
- *2024.02*: &nbsp;![alt text](new-3.gif) Our paper "TECD_Attention: Texture-enhanced and cross-domain attention modeling for visual place recognition" has been accepted by《Computer Vision and Image Understanding》 ! 


#  Honors and Awards
- *2024.08* 2024年省级大学生创新创业训练计划项目拟立项 (指导老师)
- *2024.06* 2024年（第十届）全国大学生统计建模大赛山东赛区一等奖 (指导老师)
- *2019.12* Best Paper Finalist in 2019 IEEE-ROBIO conference 
- *2019.10* Excellent Doctoral Scholarship in 2019 Tongji University
- *2023.06* Outstanding graduates in 2023 Tongji University

#  Experiences
- *2023.10 - present*, Teacher, School of Mechanical Engineering, Qilu University of Technology (Shandong Academy of Sciences). 
- *2018.09 - 2023.06*, Ph.D candidate, Tongji University. 

#  Teams
<details>
<summary>展开查看</summary>
<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="flex: 0 0 120px;">
    <img src="_pages/lzy-1.png" alt="lzy-1's photo" width="120">
  </div>
  <div style="margin-left: 20px;">
    <p><strong>Zhenyu Li</strong> (PI)  Qilu University of Technology(Shandong Academy of Sciences)</p>
    <p><strong>Ph.D, Tongji University</strong></p>
    <p><strong>Email:</strong> <a href="mailto:lizhenyu@ieee.org">lizhenyu@ieee.org</a></p>
    <p><strong>Research Areas:</strong> Computer Vision, Intelligent Perception, Edge Computing</p>
    <p><strong>Google Scholar:</strong>https://scholar.google.com/citations?hl=en&user=VZi8rpAAAAAJ</p>
    <p><strong>GitHub:</strong>https://github.com/CV4RA</p>
  </div>
</div>
<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="flex: 0 0 120px;">
    <img src="_pages/xpj.jpg" alt="xpj's photo" width="120">
  </div>
  <div style="margin-left: 20px;">
    <p><strong>Pengjie Xu</strong></p>
    <p><strong>Postdoctoral Fellow, Shanghai Jiao Tong University</strong></p>
    <p><strong>Email:</strong> <a href="mailto:xupengjie194105@sjtu.edu.cn">xupengjie194105@sjtu.edu.cn</a></p>
    <p><strong>Research Areas:</strong> Machine Learning, Robotics Systems</p>
  </div>
</div>
<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="flex: 0 0 120px;">
    <img src="_pages/dzj.png" alt="dzj's photo" width="120">
  </div>
  <div style="margin-left: 20px;">
    <p><strong>Zhaojun Deng</strong></p>
    <p><strong>Postdoctoral Fellow, Tongji University</strong></p>
    <p><strong>Email:</strong> <a href="mailto:dengzhaojun@tongji.edu.cn">dengzhaojun@tongji.edu.cn</a></p>
    <p><strong>Research Areas:</strong> Machine Learning, Photoelectric Measuring Technology</p>
  </div>
</div>
<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="flex: 0 0 120px;">
    <img src="_pages/pwh.jpg" alt="pwh's photo" width="120">
  </div>
  <div style="margin-left: 20px;">
    <p><strong>Wenhao Pei</strong></p>
    <p><strong>Third year undergraduate, Qilu University of Technology(Shandong Academy of Sciences)</strong></p>
    <p><strong>Email:</strong> <a href="mailto:202201210016@stu.qlu.edu.cn">202201210016@stu.qlu.edu.cn</a></p>
    <p><strong>Research Areas:</strong>Visual Localization and Navigation, Intelligent Perception</p>
    <p><strong>Paper (2):</strong>1 SCI paper → RAL (TOP Journal); 1 SCI paper → ITITS (TOP Journal)</p>
    <p><strong>Award-winning:</strong>2 Innovation Competition Awards</p>
  </div>
</div>
<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="flex: 0 0 120px;">
    <img src="_pages/sty.png" alt="sty's photo" width="120">
  </div>
  <div style="margin-left: 20px;">
    <p><strong>Tianyi Shang</strong>(Visiting Student)</p>
    <p><strong>Third year undergraduate, Fuzhou University</strong></p>
    <p><strong>Email:</strong> <a href="mailto:832201319@fzu.edu.cn">832201319@fzu.edu.cn</a></p>
    <p><strong>Research Areas:</strong>Computer Vision, Intelligent Perception</p>
    <p><strong>Paper (3):</strong>1 SCI paper → RAL (TOP Journal); 1 SCI paper → ITITS (TOP Journal); 1 SCI paper → TAI (TOP Journal)</p>
  </div>
</div>

</details>

#  Publications 
<font color=gray face="华文新魏">(2024 submitted papers)
<details>
<summary>展开查看</summary>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR2025</div><img src='images/cvpr.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
Bridging Text and Vision: *******************

Tianyi Shang(大三本科生), **Zhenyu Li***, Pengjie Xu, Wenhao Pei(大三本科生)
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE-RAL</div><img src='images/ral.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[MambaPlace: Text-to-Point-Cloud Cross-Modal Place Recognition with Attention Mamba Mechanisms](https://arxiv.org/pdf/2408.15740), *IEEE Robotics and Automation Letters*.

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/mambaplace-text-to-point-cloud-cross-modal/visual-place-recognition-on-kitti360pose)](https://paperswithcode.com/sota/visual-place-recognition-on-kitti360pose?p=mambaplace-text-to-point-cloud-cross-modal) <a href="https://arxiv.org/pdf/2408.15740"><img src="https://img.shields.io/badge/Paper-pdf-<COLOR>.svg?style=flat-square" /></a> [![Code](https://img.shields.io/badge/GitHub-Code-lightgrey?logo=github)](https://github.com/CV4RA/MambaPlace)

Tianyi Shang(大三本科生), **Zhenyu Li***, Wenhao Pei(大三本科生)
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE-TITS</div><img src='images/its.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
Multi-Modal Attention Perception for Intelligent Vehicle Navigation using Deep Reinforcement Learning, *IEEE Transactions on Intelligent Transportation Systems*. 

[![Paper](https://img.shields.io/badge/Paper-pdf-brightgreen)](_pages/T_ITITS_MMAP_DRL_Nav (1).pdf) [![Code](https://img.shields.io/badge/GitHub-Code-lightgrey?logo=github)](https://github.com/CV4RA/MMAP-DRL-Nav)

**Zhenyu Li***, Tianyi Shang(大三本科生), Pengjie Xu, Wenhao Pei(大三本科生)
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE-TAI</div><img src='images/tai.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
CWPFormer: Towards High-performance Visual Place Recognition for Robot with Cross-weight Attention Learning, *IEEE Transactions on Artificial Intelligence*. 

[![Paper](https://img.shields.io/badge/Paper-pdf-brightgreen)](_pages/TAI_CWPFormer__Towards_High_performance_Visual_Place_Recognition_for_Robot_with_Cross_weight_Attention_Learning (2).pdf) [![Code](https://img.shields.io/badge/GitHub-Code-lightgrey?logo=github)](https://github.com/CV4RA/CWPFormer)

**Zhenyu Li***, Tianyi Shang(大三本科生), Pengjie Xu, Wenhao Pei(大三本科生)
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE-TITS</div><img src='images/itits.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
Feature-Level Knowledge Distillation for Place Recognition based on Soft-Hard Labels Teaching Paradigm, *IEEE Transactions on Intelligent Transportation Systems*. (<font color=Fuchsia>accepted</font>)

[![Paper](https://img.shields.io/badge/Paper-pdf-brightgreen)](_pages/itits.pdf) [![Code](https://img.shields.io/badge/GitHub-Code-lightgrey?logo=github)](https://github.com/CV4RA/ASHT-KD)

**Zhenyu Li***,Tianyi Shang(大三本科生), Pengjie Xu, Zhaojun Deng, and Ruirui Zhang
</div>
</div>
<font color=gray face="华文新魏">(2024 publicated papers)
<details>
<summary>展开查看</summary>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE-TII</div><img src='images/tii.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Towards Robust Visual Place Recognition for Mobile Robots with an End-to-end Dark-enhanced Net](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10726589), *IEEE Transactions on Industrial Informatics*. 

[![Paper](https://img.shields.io/badge/Paper-pdf-brightgreen)](_pages/FINALVERSION.pdf) [![Code](https://img.shields.io/badge/GitHub-Code-lightgrey?logo=github)](https://github.com/CV4RA/Dark-enhanced-VPR-Net)

**Zhenyu Li***, Tianyi Shang (大三本科生), Pengjie Xu, Zhaojun Deng, and Ruirui Zhang
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVIU</div><img src='images/cviu.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Pyramid transformer-based triplet hashing for robust visual place recognition](https://www.sciencedirect.com/science/article/pii/S1077314224002480), *Computer Vision and Image Understanding*. (CCF-B)

**Zhenyu Li*** and Pengjie Xu
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EAAI</div><img src='images/eaai.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Reinforcement learning-based distributed impedance control of robots forcompliant operation in tight interaction tasks](https://authors.elsevier.com/c/1jPYU3OWJ98fVS), *Engineering Applications of Artificial Intelligence*. 

Pengjie Xu, **Zhenyu Li**, Xun Liu, Tianrui Zhao, Lin Zhang, Yanzheng Zhao
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Neurocomputing</div><img src='images/CSPFormer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[CSPFormer: A Cross-Spatial Pyramid Transformer for Visual Place Recognition](https://www.sciencedirect.com/science/article/pii/S0925231224002431), *Neurocomputing*. (CCF-C)

**Zhenyu Li***, Pengjie Xu
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVIU</div><img src='images/TECD.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[TECD_Attention: Texture-enhanced and cross-domain attention modeling for visual place recognition](https://www.sciencedirect.com/science/article/pii/S1077314224000109), *Computer Vision and Image Understanding*. (CCF-B)

**Zhenyu Li***, Zhenbiao Dong
</div>
</div>

#  Contact me
